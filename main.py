# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18O1wmCeYtuCGIEJlBpZYvu5pEtiadCNT
"""

import os
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from data_preprocessing import get_transforms, prepare_data
from model import initialize_model
from utils import save_checkpoint, test_model
import torch
from tqdm import tqdm

# Paths and Hyperparameters
images_dir = '/path/to/images'
annotations_dir = '/path/to/annotations'
checkpoint_dir = './checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

num_classes = 120
learning_rate = 0.001
batch_size = 32
num_epochs = 15
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Prepare data
train_transforms, test_transforms = get_transforms()
train_dataset, test_dataset = prepare_data(images_dir, annotations_dir, train_transforms, test_transforms, split_ratio)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Initialize model
model, criterion, optimizer = initialize_model(num_classes, learning_rate)
model = model.to(device)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)
scaler = torch.cuda.amp.GradScaler() # Mixed precision scaler

# Training loop
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    # Initialize tqdm progress bar for the current epoch
    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", leave=False)

    for batch_idx, (X_batch, y_batch) in enumerate(loop):

        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass with automatic mixed precision
        with torch.cuda.amp.autocast():
          y_pred = model(X_batch)
          loss = criterion(y_pred, y_batch)

       # Backward pass and optimizer step with scaler
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()

        # Update tqdm description with running loss
        loop.set_postfix(loss=running_loss / (batch_idx + 1))

    # calculate the average loss for the epoch
    epoch_loss = running_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}")

    # Apply ReduceLROnPlateau scheduler after each epoch, passing the average loss
    scheduler.step(epoch_loss)

    # Save checkpoint
    checkpoint_filename = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth")
    save_checkpoint(model, optimizer, scheduler, epoch, epoch_loss, checkpoint_filename)

# Testing
avg_loss, accuracy, f1, class_report = test_model(model, test_loader, criterion)
print(f"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, Class Report: {class_report}")